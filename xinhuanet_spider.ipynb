{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_index_pages():\n",
    "    \"\"\"\n",
    "    选取导航栏中的不同的话题\n",
    "    \"\"\"\n",
    "    r = requests.get('http://www.news.cn/')\n",
    "    tree = etree.HTML(r.text)\n",
    "    index_spans = []\n",
    "    index_pages = []\n",
    "    index_spans.extend(tree.xpath('/html/body/div[1]/div/div[2]/div/div[3]/div[1]/a'))\n",
    "    index_spans.extend(tree.xpath('/html/body/div[1]/div/div[2]/div/div[3]/div[2]/a'))\n",
    "\n",
    "    for index_page in index_spans:\n",
    "        if index_page.text not in ['视频', '图片', '高层']:\n",
    "            index_pages.append(index_page.get('href'))\n",
    "\n",
    "    return index_pages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    try:\n",
    "        news = ''\n",
    "        r = requests.get(url)\n",
    "        r.encoding = r.apparent_encoding\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        title = soup.title.text.strip()  # 获得标题\n",
    "        title += '!!!!'  # 区分标记\n",
    "        news += title\n",
    "        # 找到所有div(块),其中id为detail\n",
    "        for x in soup.find_all('div', {'id': ['detail']}):\n",
    "            for y in x.find_all('p'):\n",
    "                text = y.text.strip()  # 得到文本, strip()去除空格\n",
    "                news += text\n",
    "        return news\n",
    "    except:\n",
    "        print(\"爬取失败\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_url(url):\n",
    "    try:\n",
    "        news_list = []  # 空列表\n",
    "        r = requests.get(url)  # 解析种子网页\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')  # 利用BeautifulSoup库解析\n",
    "        tags = soup.find_all('a')  # 找到所有锚/超链接\n",
    "        for tag in tags:\n",
    "            news_list.append((str(tag.get('href')).strip()))  # 得到href\n",
    "        news_list = list(set(news_list))\n",
    "        return news_list\n",
    "    except:\n",
    "        print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def clean_urls(urls):\n",
    "    urls = list(set(urls))\n",
    "    cleaned_urls = []\n",
    "    for url in urls:\n",
    "        if 'www.news.cn' in url and (url.split(\".\")[-1] == 'htm' or url.split(\".\")[-1] == 'html'):\n",
    "            cleaned_urls.append(url)\n",
    "\n",
    "    return cleaned_urls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if len(text) < 30 or text[-4:] == '!!!!':\n",
    "        return\n",
    "    cleaned_text = re.sub(r'\\s+', '', text)  # 去除空格、换行符等\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['http://www.news.cn/politics/xxjxs/index.htm',\n 'http://www.news.cn/politics/leaders/index.htm',\n 'http://www.news.cn/politics/index.html',\n 'http://www.news.cn/politics/xhrs/index.html',\n 'http://www.news.cn/world/index.html',\n 'http://www.news.cn/fortune/index.htm',\n 'http://www.news.cn/comments/index.html',\n 'http://www.news.cn/gangao/index.html',\n 'http://www.news.cn/tw/index.html',\n 'http://www.news.cn/sikepro/index.html',\n 'http://www.news.cn/world/globalink/index.html',\n 'http://education.news.cn/index.htm',\n 'http://www.news.cn/tech/index.html',\n 'http://www.news.cn/science/index.htm',\n 'http://sports.news.cn/index.htm',\n 'http://www.news.cn/culture/',\n 'http://www.news.cn/health/index.html',\n 'http://www.news.cn/milpro/index.htm',\n 'http://www.news.cn/talking/index.html',\n 'http://www.news.cn/politics/zywj/index.htm',\n 'http://www.news.cn/money/index.html',\n 'http://www.news.cn/auto/index.html',\n 'http://www.news.cn/food/index.html',\n 'http://www.news.cn/house/index.html',\n 'http://www.news.cn/info/index.html',\n 'http://xczx.news.cn/',\n 'http://www.news.cn/info/xbsyzg/index.html',\n 'http://city.news.cn/',\n 'http://www.news.cn/travel/index.html',\n 'http://www.news.cn/energy/index.html',\n 'http://www.news.cn/expo/index.html',\n 'http://www.news.cn/caipiao/index.html',\n 'http://www.news.cn/ent/index.html',\n 'http://www.news.cn/fashion/index.html',\n 'http://www.news.cn/book/index.html',\n 'http://www.news.cn/gongyi/index.html',\n 'http://www.news.cn/shuhua/index.html',\n 'http://www.news.cn/silkroad/index.html',\n 'http://www.news.cn/asia/chinese/index.html',\n 'http://www.news.cn/finance/',\n 'http://www.news.cn/finance/tjjd/index.htm']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_pages = get_index_pages()\n",
    "index_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1047"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for page in index_pages:\n",
    "    urls.extend(get_all_url(page))\n",
    "valid_urls = clean_urls(urls)\n",
    "len(valid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取失败\n"
     ]
    }
   ],
   "source": [
    "f = open(\"foo.txt\", \"w\", encoding=\"UTF-8\")\n",
    "for i in valid_urls:\n",
    "    text = get_text(i)\n",
    "    text = clean_text(text) if text else None\n",
    "    if text:\n",
    "        f.write(i)\n",
    "        f.write(' ')\n",
    "        f.write(text)\n",
    "        f.write(\"\\n\")\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}